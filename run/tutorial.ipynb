{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RefAV Tutorial\n",
    "To start, we separate the scenario mining ground truth annotations into separate log folders.\n",
    "\n",
    "At the same time, we create the ground truth .pkl files we can use to evaluate performance later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from refAV.paths import SM_DATA_DIR\n",
    "from refAV.dataset_conversion import separate_scenario_mining_annotations, create_gt_mining_pkls_parallel\n",
    "\n",
    "sm_val_feather = Path('av2_sm_downloads/scenario_mining_val_annotations.feather')\n",
    "separate_scenario_mining_annotations(sm_val_feather, SM_DATA_DIR / 'val')\n",
    "create_gt_mining_pkls_parallel(sm_val_feather, SM_DATA_DIR / 'val', num_processes=max(1, int(.9*os.cpu_count())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RefAV takes any base set of predicted tracks runs a set of filtering operations to identify the relevant portions of each track.\n",
    "The baseline uses the track predictions from the winner of the 2024  AV2 End-to-End Forecasting challenge: Le3DE2E. This block downloads the track predictions for the val set from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from refAV.dataset_conversion import pickle_to_feather\n",
    "from refAV.paths import SM_PRED_DIR, AV2_DATA_DIR\n",
    "from pathlib import Path\n",
    "\n",
    "repo_id = \"CainanD/AV2_Tracker_Predictions\"\n",
    "filename = \"Le3DE2E_tracking_predictions_val.pkl\"\n",
    "tracker_predictions_dir = 'tracker_predictions'\n",
    "\n",
    "hf_hub_download(repo_id, filename, repo_type='dataset', local_dir=tracker_predictions_dir)\n",
    "\n",
    "tracking_val_predictions = Path(tracker_predictions_dir + '/' + filename)\n",
    "\n",
    "pickle_to_feather(AV2_DATA_DIR / 'val', tracking_val_predictions, SM_PRED_DIR / 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RefAV works by constructing compositional functions that can be used to define a scenario.\n",
    "\n",
    "Here is an example of using the compositional functions to define a scenario corresponding \n",
    "to a \"moving vehicle behind another vehicle being crossed by a jaywalking pedestrian'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from refav.utils import *\n",
    "from refav.paths import SM_PRED_DIR, SM_DATA_DIR\n",
    "from IPython.display import Video\n",
    "\n",
    "dataset_dir = SM_DATA_DIR / 'val'\n",
    "output_dir = SM_PRED_DIR / 'val'\n",
    "log_id = '0b86f508-5df9-4a46-bc59-5b9536dbde9f'\n",
    "log_dir = dataset_dir / log_id\n",
    "\n",
    "description = 'vehicle behind another vehicle being crossed by a jaywalking pedestrian'\n",
    "\n",
    "peds = get_objects_of_category(log_dir, category='PEDESTRIAN')\n",
    "peds_on_road = on_road(peds, log_dir)\n",
    "jaywalking_peds = scenario_not(at_pedestrian_crossing)(peds_on_road, log_dir)\n",
    "\n",
    "vehicles = get_objects_of_category(log_dir, category='VEHICLE')\n",
    "moving_vehicles = scenario_and([in_drivable_area(vehicles, log_dir), scenario_not(stationary)(vehicles, log_dir)])\n",
    "crossed_vehicles = being_crossed_by(moving_vehicles, jaywalking_peds, log_dir)\n",
    "behind_crossed_vehicle = get_objects_in_relative_direction(crossed_vehicles, moving_vehicles, log_dir,\n",
    "\t\t\t\t\t\t\t\t\t\t\tdirection='backward', max_number=1, within_distance=25)\n",
    "\n",
    "#Output scenario outputs a .pkl and .mp4 for the predicted tracks during that scenario\n",
    "output_scenario(behind_crossed_vehicle, description, log_dir, output_dir, visualize=True)\n",
    "\n",
    "Video('output/experiments/val/0b86f508-5df9-4a46-bc59-5b9536dbde9f/scenario visualizations/vehicle behind another vehicle being crossed by a jaywalking pedestrian_n6.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to define a scenario, let's let an LLM do it for us.\n",
    "\n",
    "This tutorial supports three different LLMs:\n",
    "1. [qwen-2-5-7b](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)\n",
    "2. [gemini-2-0-flash-thinking](https://aistudio.google.com/prompts/new_chat)\n",
    "3. [claude-3-5-sonnet](https://www.anthropic.com/api)\n",
    "\n",
    "Qwen is an open-source, open-weight model that is run locally. Gemini requires a free API key through AI Studio. Claude requires a paid API key through Anthropic. Since Claude is a paid model, we provide the predicted scenario definitions in the output/llm_scenario_predictions/claude-sonnnet-3-5 folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from refav.scenario_prediction import predict_scenario_from_description\n",
    "from refav.paths import LLM_DEF_DIR\n",
    "\n",
    "prompt = 'vehicle heading towards ego from the side while at an intersection'\n",
    "model = 'gemini-2-0-flash-thinking'\n",
    "\n",
    "predict_scenario_from_description(prompt, LLM_DEF_DIR, model)\n",
    "\n",
    "with open(LLM_DEF_DIR/model/(prompt + '.txt'), 'r') as def_file:\n",
    "    definition_text = def_file.read()\n",
    "    print(definition_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the basics out of the way, let's run evaluation on the entire validation dataset.\n",
    "The create_base_prediction function calls the LLM scenario definition generator and the\n",
    " runs the defintion to find instance of the prompt.\n",
    "  It can take quite a bit of time to go through all of the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from refav.paths import SM_PRED_DIR, LLM_DEF_DIR\n",
    "from refav.eval import create_baseline_prediction\n",
    "\n",
    "log_prompt_input_path = Path('av2_sm_downloads/log_prompt_pairs_val.json')\n",
    "with open(log_prompt_input_path, 'rb') as f:\n",
    "\tlog_prompts = json.load(f)\n",
    "\n",
    "method_name = 'qwen-2-5-7b'\n",
    "for i, (log_id, prompts) in enumerate(log_prompts.items()):\n",
    "\tprint(log_id)\n",
    "\tfor prompt in tqdm(prompts, desc=f'{i}/{len(log_prompts)}'):\n",
    "\t\tcreate_baseline_prediction(prompt, log_id, SM_PRED_DIR / 'val', LLM_DEF_DIR, method_name=method_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combine_matching_pkls function will combine all prediction and ground truth .pkl files into a single .pkl file. This is the .pkl file that is used for submission to the leaderboard. Running evaluate_pkls will the predicted tracks across four metrics: HOTA-Temporal, HOTA, timestamp-level F1, and scenario-level F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from refav.paths import SM_DATA_DIR, SM_PRED_DIR\n",
    "from refav.eval import combine_pkls, evaluate_pkls\n",
    "\n",
    "eval_output_dir = Path(f'output/evaluation/val')\n",
    "combine_pkls(SM_DATA_DIR / 'val', SM_PRED_DIR / 'val', eval_output_dir, method_name=method_name)\n",
    "metrics = evaluate_pkls(eval_output_dir / f'{method_name}_predictions.pkl', eval_output_dir / 'combined_gt.pkl')\n",
    "\n",
    "print_indented_dict(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the evaluate function executes successfully, your predictions .pkl file is ready to submit to the EvalAI server for evaluation. Create a profile at EvalAI [EvalAI](https://eval.ai/) in to receive an account token. This code will submit to the validation set. This should take about 10-30 minutes to evaluate, depending on the number of predicted tracks.\n",
    "\n",
    "```bash\n",
    "pip install evalai\n",
    "evalai set_token <EvalAI_account_token>\n",
    "evalai challenge 2469 phase 4899 submit --file output/evaluation/val/combined_predictions.pkl --large\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
